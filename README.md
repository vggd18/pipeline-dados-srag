# Desafio de Engenharia de Dados - GGMA (Sec. Sa√∫de Recife)

Este reposit√≥rio cont√©m a minha solu√ß√£o para o desafio de Engenharia de Dados J√∫nior da Secretaria de Sa√∫de do Recife. O projeto consiste em um pipeline ETL (Extract, Transform, Load) completo, constru√≠do para ser robusto, eficiente, escal√°vel e observ√°vel.

O pipeline extrai dados de S√≠ndrome Respirat√≥ria Aguda Grave (SRAG) do OpenDataSUS, aplica um rigoroso processo de transforma√ß√£o modularizado e os carrega em um banco de dados anal√≠tico local, pronto para as consultas SQL solicitadas.

## üöÄ Justificativa da Stack de Tecnologias

A escolha da stack foi um pilar central deste projeto, alinhada √† filosofia do desafio de priorizar "qualidade ao inv√©s de complexidade". A stack escolhida foi **Polars + DuckDB**.

### Por que Polars + DuckDB?

Esta √© a "combina√ß√£o perfeita" para este escopo.

* **Polars** √© um framework de DataFrame *multi-thread* escrito em Rust. Ele oferece performance muito superior ao Pandas tradicional e possui um poderoso "Lazy Mode" que permite processar datasets maiores que a RAM dispon√≠vel, como foi implementado neste projeto.
* **DuckDB** √© um banco de dados SQL **anal√≠tico (OLAP)**, e n√£o transacional (OLTP). Ao contr√°rio do Postgres (OLTP row-based), o DuckDB √© *column-based*, otimizado para a velocidade de consultas anal√≠ticas (`GROUP BY`, `AVG`, etc.).

Juntos, eles se comunicam via **Apache Arrow** (`pyarrow`), permitindo transfer√™ncias de dados *zero-copy* (sem c√≥pia) da mem√≥ria do Polars para o DuckDB, resultando na carga mais r√°pida poss√≠vel.

### Por que N√ÉO as Alternativas?

* **Por que n√£o Pandas?** Pandas √© (majoritariamente) *single-thread* e n√£o possui um otimizador de consultas "Lazy" t√£o robusto. Para um dataset com +250 mil linhas e 194 colunas, ele estaria no seu limite de performance e mem√≥ria, sendo uma escolha tecnicamente inferior.
* **Por que n√£o Spark?** Spark √© uma "bala de canh√£o para matar uma mosca". √â um framework de computa√ß√£o *distribu√≠da*, projetado para clusters e terabytes de dados. Instanciar uma sess√£o Spark (com sua JVM) para ler um √∫nico arquivo Parquet seria um exemplo de *over-engineering* (complexidade desnecess√°ria).
* **Por que n√£o Postgres ou SQLite?** Optei por n√£o usar bancos de dados tradicionais como Postgres ou SQLite por dois motivos. Primeiro, eles s√£o bancos **OLTP** (transacionais), otimizados para opera√ß√µes linha a linha e ineficientes para as consultas anal√≠ticas (`GROUP BY`) do desafio. Segundo, o Postgres exigiria a configura√ß√£o de um ambiente de servidor (provavelmente via Docker), o que desviaria o foco do **tratamento dos dados** para a **arquitetura de infraestrutura**. O **DuckDB (OLAP)** foi a escolha correta por ser *serverless*, *column-based* e otimizado para a performance anal√≠tica que o desafio pedia, permitindo foco total na qualidade dos dados.

## üèÉ Como Executar o Projeto

1.  Clone este reposit√≥rio:
    ```bash
    git clone https://github.com/vggd18/pipeline-dados-srag.git
    cd pipeline-dados-srag
    ```

2.  Crie um ambiente virtual e instale as depend√™ncias:
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # No Windows: .\venv\Scripts\activate
    pip install -r requirements.txt
    ```

3.  **Configura√ß√£o:** Copie o arquivo de configura√ß√£o de exemplo para criar seu arquivo `.env` local:
    ```bash
    cp .env.example .env
    ```
    *(Os valores padr√£o no `.env` devem funcionar para este desafio).*

4.  Execute o pipeline ETL completo:
    ```bash
    python etl_pipeline.py
    ```
    O script ir√° (1) baixar os dados, (2) process√°-los em modo *Lazy* e (3) carregar o resultado no arquivo `data/srag.duckdb`. A sa√≠da no terminal mostrar√° os logs de cada etapa e suas dura√ß√µes.

## üß™ Testando a Qualidade e as Consultas

Para garantir a robustez e facilitar a valida√ß√£o, o projeto inclui:

1.  **Teste P√≥s-ETL:** A execu√ß√£o do `python etl_pipeline.py` inclui uma etapa final (`test_database()`) que verifica se a tabela `srag` foi criada com sucesso e loga a contagem de linhas e colunas carregadas.

2.  **Script de Execu√ß√£o de Consultas:** Para validar as consultas SQL anal√≠ticas, voc√™ pode usar o script dedicado:

    ```bash
    python test_queries.py
    ```
    Este script se conectar√° ao banco `data/srag.duckdb` (criado pelo ETL) e imprimir√° os resultados das consultas encontradas na pasta `/sql/`.

---

## üìã Relat√≥rio do Desafio

Esta se√ß√£o cumpre o requisito de "breve relat√≥rio" do processo seletivo.

### 1. Dados

* **Descri√ß√£o dos Dados:** Os dados escolhidos foram os registros de S√≠ndrome Respirat√≥ria Aguda Grave (SRAG) **hospitalizados**, disponibilizados pelo Minist√©rio da Sa√∫de via OpenDataSUS.
* **Fonte(s) de Dados:**
    * [P√°gina de Recursos (OpenDataSUS)](https://opendatasus.saude.gov.br/dataset/srag-2021-a-2024): P√°gina principal do dataset SRAG.
    * [Arquivo de Dados (Parquet 2024)](https://opendatasus.saude.gov.br/gl/dataset/srag-2021-a-2024/resource/0df883b2-4c71-4e7d-ab77-1767e6b956c5): Link direto para o arquivo `.parquet` de 2024 utilizado neste projeto.
    * [Dicion√°rio de Dados (PDF)](https://opendatasus.saude.gov.br/dataset/srag-2021-a-2024/resource/3135ac9c-2019-4989-a893-2ed50ebd8e68): Documento oficial descrevendo todas as colunas do dataset.
* **Justificativa da Escolha:**
    1.  **Relev√¢ncia:** Sendo a vaga para a Secretaria de Sa√∫de do Recife, utilizar dados de sa√∫de p√∫blica (SRAG) √© diretamente relevante para o dom√≠nio da organiza√ß√£o.
    2.  **Escopo:** Conforme a orienta√ß√£o do desafio de focar em "qualidade" , optei por utilizar apenas os dados de 2024. Este escopo (+250.000 registros) √© robusto o suficiente para provar a efici√™ncia do pipeline (especialmente a otimiza√ß√£o de mem√≥ria) sem adicionar complexidade desnecess√°ria.

### 2. Pipeline (Extra√ß√£o e Transforma√ß√£o)


O pipeline foi constru√≠do seguindo boas pr√°ticas de engenharia, com foco em **Modularidade**, **Observabilidade** e **Efici√™ncia de Mem√≥ria** (Lazy Mode).

* **Configura√ß√£o Externa:** Todas as constantes (URLs, caminhos, logs) s√£o gerenciadas em um arquivo `config.py` e carregadas via `.env` (usando `python-dotenv`). Isso permite f√°cil reconfigura√ß√£o sem alterar o c√≥digo do pipeline e segue a boa pr√°tica de separar configura√ß√£o de c√≥digo.

* **Observabilidade:** Um decorator `@log_step` foi implementado para registrar o in√≠cio, fim e **dura√ß√£o (em segundos)** de cada etapa principal. Logs detalhados (ex: contagem de registros e colunas, tamanho do banco) s√£o registrados durante a execu√ß√£o para permitir o rastreamento do progresso e resultados.

* **Extra√ß√£o (E):** A fun√ß√£o `extract()` usa `pl.scan_parquet()` para escanear o arquivo do S3 sem carreg√°-lo na mem√≥ria, apenas lendo o esquema.

* **Transforma√ß√£o (T):** Esta √© a etapa central e foi modularizada em uma sequ√™ncia de fun√ß√µes at√¥micas que operam no `LazyFrame` (plano de execu√ß√£o):
    1.  **`rename_columns()`**: Padroniza todas as colunas para `snake_case` min√∫sculo.
    2.  **`convert_data_types()`**: Converte colunas de data (`str` -> `Date`), aplica tipagem booleana (`1/2/9` -> `True/False/None`) e converte colunas de alta cardinalidade para `Categorical`.
    3.  **`map_categorical_codes()`**: Realiza o polimento sem√¢ntico, mapeando c√≥digos cr√≠pticos (ex: `classi_fin = '5'`) para labels leg√≠veis (ex: `"SRAG por covid-19"`).
    4.  **`clean_and_deduplicate()`**: Aplica a l√≥gica de neg√≥cio central: deduplica pela chave prim√°ria (`nu_notific`), sanitiza colunas string (`strip/lowercase`) e filtra o dataset para manter apenas casos v√°lidos (`hospital = True`).
    5.  **`get_valid_columns()`**: Como etapa final de qualidade, esta fun√ß√£o remove colunas com nulidade > 70%, mas aplica um **trade-off** estrat√©gico para manter colunas de comorbidade (ex: `renal`, `obesidade`) que s√£o analiticamente importantes, mesmo que esparsas.

* **Carregamento (L):** O carregamento no banco SQL DuckDB √© feito de forma nativa e otimizada:
    1.  Uma conex√£o direta com o banco (`duckdb.connect()`) √© aberta.
    2.  O plano "Lazy" final do Polars (`df_final`) √© "registrado" (`con.register()`) no DuckDB, usando Apache Arrow para uma transfer√™ncia *zero-copy*.
    3.  O DuckDB executa o plano completo e realiza a carga em massa com `CREATE OR REPLACE TABLE srag AS SELECT ...`.

### 3. Consultas SQL

As consultas foram projetadas para demonstrar duas abordagens anal√≠ticas diferentes: uma an√°lise "Micro" (focada em dados de Recife) e uma "Macro" (focada em padr√µes epidemiol√≥gicos).

* **Consulta 1: `sql/consulta_1.sql` - An√°lise Micro: Perfil de Gravidade por Faixa Et√°ria em Recife**
    * **Descri√ß√£o:** Esta consulta filtra os casos de SRAG por COVID-19 (2024) para residentes do Recife (c√≥digo `261160`). Ela agrupa os pacientes em faixas et√°rias (`pediatrico`, `adulto`, `idoso`) e calcula o total de casos, a taxa de UTI e a taxa de letalidade para cada grupo.
    * **Justificativa Anal√≠tica:** Sendo a vaga para a Sec. de Sa√∫de do Recife, esta consulta gera um recorte de **alta relev√¢ncia local**. Ela responde: "Qual foi o impacto da COVID-19 em *nossos* mun√≠cipios?". O insight (baseado nos 73 casos) foi claro: o n√∫mero de casos graves foi baixo em 2024, e a letalidade se concentrou de forma extrema no grupo 'idoso' (42.8%), um dado valioso para a vigil√¢ncia epidemiol√≥gica local.

* **Consulta 2: `sql/consulta_2.sql` - An√°lise Macro: Impacto do Ac√∫mulo de Comorbidades na Gravidade (Brasil)**
    * **Descri√ß√£o:** Esta consulta usa uma **Common Table Expression (CTE)** para uma an√°lise de risco avan√ßada. O CTE primeiro calcula o *n√∫mero total de comorbidades* (diabetes, cardiopatia, etc.) para cada paciente de COVID-19 em *todo o dataset*. A consulta principal agrupa os pacientes por essa contagem (0, 1, 2, 3+) e calcula a `taxa_uti` e a `taxa_letalidade` para cada grupo de risco.
    * **Justificativa Anal√≠tica:** Ao contr√°rio da Consulta 1, uma an√°lise de comorbidades no recorte de Recife (N=73) seria estatisticamente fraca. Por isso, esta consulta muda para uma abordagem **"Macro" (n√≠vel nacional)** para identificar um padr√£o epidemiol√≥gico robusto. O insight foi poderoso e claro: **o risco aumenta linearmente com o n√∫mero de comorbidades**. A taxa de letalidade saltou de **10.9%** (para 0 comorbidades) para **27.3%** (para 3+ comorbidades). Esta consulta demonstra o valor do pipeline ETL (que limpou +10 colunas booleanas) e a capacidade de ajustar o escopo da an√°lise (Micro vs. Macro) para gerar insights estatisticamente v√°lidos.