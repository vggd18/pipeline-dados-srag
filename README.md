# Desafio de Engenharia de Dados - GGMA (Sec. Sa√∫de Recife)

Este reposit√≥rio cont√©m a minha solu√ß√£o para o desafio de Engenharia de Dados J√∫nior da Secretaria de Sa√∫de do Recife. O projeto consiste em um pipeline ETL (Extract, Transform, Load) que processa dados de S√≠ndrome Respirat√≥ria Aguda Grave (SRAG) do OpenDataSUS.

O pipeline √© constru√≠do com **Polars** e **DuckDB**, utilizando uma abordagem "Lazy" (pregui√ßosa) para garantir alta performance e baixo uso de mem√≥ria, permitindo o processamento de datasets maiores que a RAM dispon√≠vel.

## üöÄ Tecnologias Utilizadas

  * **Python 3.10+**
  * **Polars:** Para extra√ß√£o e transforma√ß√£o de dados em alta performance (Lazy Mode).
  * **DuckDB:** Como banco de dados SQL anal√≠tico (OLAP) open-source.
  * **s3fs:** Para permitir que o Polars leia (`scan_parquet`) arquivos diretamente do S3.
  * **PyArrow:** Como a "ponte" de interoperabilidade zero-copy entre Polars e DuckDB.

## üèÉ Como Executar o Projeto

1.  Clone este reposit√≥rio:

    ```bash
    git clone https://github.com/vggd18/pipeline-dados-srag.git
    cd pipeline-dados-srag
    ```

2.  Crie um ambiente virtual (recomendado) e instale as depend√™ncias:

    ```bash
    python3 -m venv venv
    source venv/bin/activate  # No Windows: .\venv\Scripts\activate
    pip install -r requirements.txt
    ```

3.  Execute o pipeline ETL:
    ```bash
    python etl_pipeline.py
    ```

    O script ir√° baixar os dados, process√°-los e carregar o resultado no arquivo `data/srag.duckdb`.

4.  (Opcional) Para verificar o banco de dados, voc√™ pode usar a CLI do DuckDB:

    ```bash
    duckdb data/srag.duckdb
    ```

    E ent√£o rodar suas consultas SQL (ex: `SELECT * FROM srag LIMIT 5;`).

-----

## üìã Relat√≥rio do Desafio

Esta se√ß√£o cumpre o requisito de "breve relat√≥rio" do processo seletivo.

### 1\. Dados

  * **Descri√ß√£o dos Dados:**
    Os dados escolhidos foram os registros de S√≠ndrome Respirat√≥ria Aguda Grave (SRAG) **hospitalizados**, disponibilizados pelo Minist√©rio da Sa√∫de via OpenDataSUS. Este conjunto de dados inclui informa√ß√µes demogr√°ficas, sintomas, fatores de risco, dados de vacina√ß√£o (COVID-19 e Influenza), interna√ß√£o, uso de UTI e resultados laboratoriais.

  * **Fonte(s) de Dados:**

      * **P√°gina de Recurso:** [Dataset Open Data SUS](https://opendatasus.saude.gov.br/dataset/srag-2021-a-2024)
      * **Link Direto (Parquet 2024):** [Link Direto Parquet](https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SRAG/2024/INFLUD24-26-06-2025.parquet)
      * **Dicion√°rio de Dados:** [Dicionario de Dados 2019 a 2025](https://opendatasus.saude.gov.br/dataset/39a4995f-4a6e-440f-8c8f-b00c81fae0d0/resource/3135ac9c-2019-4989-a893-2ed50ebd8e68/download/dicionario-de-dados-2019-a-2025.pdf)

  * **Justificativa da Escolha:**
    Escolhi este dataset por tr√™s motivos:

    1.  **Relev√¢ncia:** Sendo uma vaga para a Secretaria de Sa√∫de do Recife, utilizar dados p√∫blicos de sa√∫de (SRAG/COVID-19) √© diretamente relevante para o dom√≠nio de neg√≥cio da organiza√ß√£o.
    2.  **Riqueza:** O dataset possui 194 colunas, o que permitiu demonstrar uma variedade de t√©cnicas de transforma√ß√£o (tipagem, sanitiza√ß√£o, mapeamento sem√¢ntico, etc.).
    3.  **Escopo:** Conforme a orienta√ß√£o do desafio de focar em "qualidade ao inv√©s de quantidade", optei por utilizar apenas os dados de 2024. Este escopo (+250.000 registros) √© robusto o suficiente para provar a efici√™ncia do pipeline, permitindo uma entrega polida e bem documentada.

### 2\. Pipeline (Extra√ß√£o e Transforma√ß√£o)

  * **Extra√ß√£o (E):** A extra√ß√£o √© feita em Python usando o modo "Lazy" do Polars. O m√©todo `pl.scan_parquet()` √© usado para "escanear" o arquivo `.parquet` diretamente do S3. Isso n√£o carrega dados na mem√≥ria, apenas l√™ o esquema e prepara um plano de execu√ß√£o.

  * **Transforma√ß√£o (T):** Todas as etapas de transforma√ß√£o s√£o encadeadas em um √∫nico plano "Lazy", o que garante o m√≠nimo uso de mem√≥ria e m√°xima otimiza√ß√£o:

    1.  **Renomea√ß√£o:** Todas as 194 colunas foram convertidas para `snake_case` (min√∫sculas) para padroniza√ß√£o.
    2.  **Tipagem Robusta:**
          * **Datas:** Colunas de data (ex: `dt_notific`) foram convertidas de string para `pl.Date`, usando `str.to_datetime(strict=False)` para garantir que formatos mistos ou nulos n√£o quebrassem o pipeline.
          * **N√∫meros:** Colunas num√©ricas (ex: `nu_idade_n`) foram convertidas para `pl.Int32(strict=False)`.
          * **Identificadores:** Colunas que s√£o c√≥digos (ex: `co_mun_not`, `nu_notific`) foram mantidas como `String` para preservar zeros √† esquerda.
    3.  **Mapeamento Sem√¢ntico:** Para melhorar a legibilidade para o analista, os dados foram "polidos":
          * **Booleanos:** Colunas que usam o padr√£o `1-Sim`, `2-N√£o`, `9-Ignorado` (ex: `febre`, `uti`) foram convertidas para o tipo `Boolean` (`True`, `False`, `None`).
          * **(WIP) Dicion√°rio de Dados:** Colunas categ√≥ricas ser√£o mapeadas de seus c√≥digos para valores leg√≠veis.
    4.  **Sanitiza√ß√£o:** Todas as colunas `String` restantes passaram por `.str.strip_chars().str.to_lowercase()` para remover espa√ßos e padronizar o case.
    5.  **Filtragem de Integridade:** O dataset foi filtrado para manter apenas registros onde `hospital == True`, alinhando os dados com a defini√ß√£o oficial de "casos hospitalizados".
    6.  **Deduplica√ß√£o:** O DataFrame final foi deduplicado pela chave prim√°ria `nu_notific`.

  * **Carregamento (L):** O carregamento no banco SQL DuckDB √© feito de forma nativa e otimizada:

    1.  Uma conex√£o direta com o banco (`duckdb.connect()`) √© aberta.
    2.  O plano "Lazy" do Polars (`df_final_lazy`) √© "registrado" (`con.register()`) no DuckDB. Esta √© uma opera√ß√£o *zero-copy* que usa o Apache Arrow para compartilhar os dados.
    3.  O DuckDB executa o plano e realiza a carga em massa com `CREATE OR REPLACE TABLE srag AS SELECT ...`, o que √© ordens de magnitude mais r√°pido do que m√©todos de inser√ß√£o tradicionais.
