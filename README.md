# Desafio de Engenharia de Dados - GGMA (Sec. Sa√∫de Recife)

Este reposit√≥rio cont√©m a minha solu√ß√£o para o desafio de Engenharia de Dados J√∫nior da Secretaria de Sa√∫de do Recife. O projeto consiste em um pipeline ETL (Extract, Transform, Load) completo, constru√≠do para ser robusto, eficiente e escal√°vel.

O pipeline extrai dados de S√≠ndrome Respirat√≥ria Aguda Grave (SRAG) do OpenDataSUS, aplica um rigoroso processo de transforma√ß√£o e limpeza, e os carrega em um banco de dados anal√≠tico local, pronto para as consultas SQL solicitadas.

## üöÄ Justificativa da Stack de Tecnologias

A escolha da stack foi um pilar central deste projeto, alinhada √† filosofia do desafio de priorizar "qualidade ao inv√©s de complexidade" . A stack escolhida foi **Polars + DuckDB**.

### Por que Polars + DuckDB?

Esta √© a "combina√ß√£o perfeita" para este escopo.

* **Polars** √© um framework de DataFrame *multi-thread* escrito em Rust. Ele oferece performance muito superior ao Pandas tradicional e possui um poderoso "Lazy Mode" que permite processar datasets maiores que a RAM dispon√≠vel, como foi implementado neste projeto.
* **DuckDB** √© um banco de dados SQL **anal√≠tico (OLAP)**, e n√£o transacional (OLTP). Ao contr√°rio do Postgres (OLTP row-based), o DuckDB √© *column-based*, otimizado para a velocidade de consultas anal√≠ticas (`GROUP BY`, `AVG`, etc.).

Juntos, eles se comunicam via **Apache Arrow** (`pyarrow`), permitindo transfer√™ncias de dados *zero-copy* (sem c√≥pia) da mem√≥ria do Polars para o DuckDB, resultando na carga mais r√°pida poss√≠vel.

### Por que N√ÉO as Alternativas?

* **Por que n√£o Pandas?** Pandas √© (majoritariamente) *single-thread* e n√£o possui um otimizador de consultas "Lazy" t√£o robusto. Para um dataset com +250 mil linhas e 194 colunas, ele estaria no seu limite de performance e mem√≥ria, sendo uma escolha tecnicamente inferior.
* **Por que n√£o Spark?** Spark √© uma "bala de canh√£o para matar uma mosca". √â um framework de computa√ß√£o *distribu√≠da*, projetado para clusters e terabytes de dados. Instanciar uma sess√£o Spark (com sua JVM) para ler um √∫nico arquivo Parquet seria um exemplo de *over-engineering* (complexidade desnecess√°ria).
* **Por que n√£o Postgres ou SQLite?** Optei por n√£o usar bancos de dados tradicionais como Postgres ou SQLite por dois motivos. Primeiro, eles s√£o bancos **OLTP** (transacionais), otimizados para opera√ß√µes linha a linha e ineficientes para as consultas anal√≠ticas (`GROUP BY`) do desafio. Segundo, o Postgres exigiria a configura√ß√£o de um ambiente de servidor (provavelmente via Docker), o que desviaria o foco do **tratamento dos dados** para a **arquitetura de infraestrutura**. O **DuckDB (OLAP)** foi a escolha correta por ser *serverless*, *column-based* e otimizado para a performance anal√≠tica que o desafio pedia, permitindo foco total na qualidade dos dados.

## üèÉ Como Executar o Projeto

1.  Clone este reposit√≥rio:
    ```bash
    git clone https://github.com/vggd18/pipeline-dados-srag.git
    cd pipeline-dados-srag
    ```

2.  Crie um ambiente virtual e instale as depend√™ncias:
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # No Windows: .\venv\Scripts\activate
    pip install -r requirements.txt
    ```

3.  Execute o pipeline ETL completo:
    ```bash
    python etl_pipeline.py
    ```
    O script ir√° (1) baixar os dados, (2) process√°-los em modo *Lazy* e (3) carregar o resultado no arquivo `data/srag.duckdb`.

4.  (Opcional) Verifique o banco de dados via CLI:
    ```bash
    duckdb data/srag.duckdb
    ```
    E ent√£o rode suas consultas.

## üß™ Testando a Qualidade e as Consultas

Para garantir a robustez e facilitar a valida√ß√£o, o projeto inclui:

1.  **Teste P√≥s-ETL:** A execu√ß√£o do `python etl_pipeline.py` inclui uma etapa final (`test_database()`) que verifica se a tabela `srag` foi criada com sucesso no banco `data/srag.duckdb`.

2.  **Script de Execu√ß√£o de Consultas:** Como alternativa √† CLI do DuckDB, voc√™ pode executar as consultas SQL anal√≠ticas (`sql/*.sql`) diretamente atrav√©s de um script Python dedicado:

    ```bash
    python test_queries.py
    ```
    Este script se conectar√° ao banco `data/srag.duckdb` (que deve ter sido criado pelo `etl_pipeline.py` primeiro) e imprimir√° os resultados das consultas encontradas na pasta `/sql/`. Isso garante que as consultas possam ser validadas independentemente da configura√ß√£o do ambiente do avaliador.
    
---

## üìã Relat√≥rio do Desafio

Esta se√ß√£o cumpre o requisito de "breve relat√≥rio" do processo seletivo.

**Nota sobre a Implementa√ß√£o:** O pipeline foi implementado seguindo boas pr√°ticas de engenharia, como a separa√ß√£o de responsabilidades em fun√ß√µes (`extract`, `transform`, `load`), o uso do m√≥dulo `logging` para rastreabilidade e a otimiza√ß√£o de mem√≥ria atrav√©s do "Lazy Mode" do Polars.

### 1. Dados

* **Descri√ß√£o dos Dados:** Os dados escolhidos foram os registros de S√≠ndrome Respirat√≥ria Aguda Grave (SRAG) **hospitalizados**, disponibilizados pelo Minist√©rio da Sa√∫de via OpenDataSUS.
* **Fonte(s) de Dados:**
    * [P√°gina de Recursos (OpenDataSUS)](https://opendatasus.saude.gov.br/dataset/srag-2021-a-2024): P√°gina principal do dataset SRAG.
    * [Arquivo de Dados (Parquet 2024)](https://opendatasus.saude.gov.br/gl/dataset/srag-2021-a-2024/resource/0df883b2-4c71-4e7d-ab77-1767e6b956c5): Link direto para o arquivo `.parquet` de 2024 utilizado neste projeto.
    * [Dicion√°rio de Dados (PDF)](https://opendatasus.saude.gov.br/dataset/srag-2021-a-2024/resource/3135ac9c-2019-4989-a893-2ed50ebd8e68): Documento oficial descrevendo todas as colunas do dataset.
* **Justificativa da Escolha:**
    1.  **Relev√¢ncia:** Sendo a vaga para a Secretaria de Sa√∫de do Recife, utilizar dados de sa√∫de p√∫blica (SRAG) √© diretamente relevante para o dom√≠nio da organiza√ß√£o.
    2.  **Escopo:** Conforme a orienta√ß√£o do desafio de focar em "qualidade" , optei por utilizar apenas os dados de 2024. Este escopo (+250.000 registros) √© robusto o suficiente para provar a efici√™ncia do pipeline (especialmente a otimiza√ß√£o de mem√≥ria) sem adicionar complexidade desnecess√°ria.

### 2. Pipeline (Extra√ß√£o e Transforma√ß√£o)

O pipeline foi constru√≠do usando o **"Lazy Mode"** (modo pregui√ßoso) do Polars para garantir o m√≠nimo uso de mem√≥ria (resolvendo o erro de `killed` por OOM) e m√°xima performance. Nenhuma etapa √© executada at√© o `con.register()` final.

* **Extra√ß√£o (E):** A extra√ß√£o √© feita com `pl.scan_parquet(URL_PATH)`. Isso apenas "escaneia" o esquema do arquivo no S3 (usando `s3fs`) sem carreg√°-lo na mem√≥ria.

* **Transforma√ß√£o (T):** Todas as transforma√ß√µes s√£o encadeadas em um √∫nico plano de execu√ß√£o "Lazy":
    1.  **Renomea√ß√£o:** Todas as 194 colunas foram convertidas para `snake_case` (min√∫sculas) para padroniza√ß√£o SQL.
    2.  **Tipagem Robusta:** As colunas foram convertidas para seus tipos corretos (ex: `Date`, `Int32`) usando `strict=False`. Isso garante que dados sujos (ex: uma data mal formatada) sejam convertidos para `NULL` em vez de quebrar o pipeline.
    3.  **Mapeamento Sem√¢ntico (Polimento):** Colunas-chave (`cs_sexo`, `evolucao`, `classi_fin`, etc.) foram mapeadas de c√≥digos cr√≠pticos (ex: `"1"`, `"2"`, `"9"`) para valores leg√≠veis (ex: `"Cura"`, `"√ìbito"`, `None`). Isso √© crucial para a usabilidade do analista.
    4.  **Mapeamento Booleano:** Dezenas de colunas (ex: `diabetes`, `cardiopati`) que usavam o padr√£o `1/2/9` foram convertidas para `True/False/None`, permitindo an√°lises SQL complexas (como a Consulta 2).
    5.  **Sanitiza√ß√£o:** Todas as colunas `String` restantes (como `co_mun_res`) foram sanitizadas (`.str.strip_chars().str.to_lowercase()`) para padronizar o case e remover espa√ßos.
    6.  **Deduplica√ß√£o:** Os registros foram deduplicados pela chave prim√°ria `nu_notific`.
    7.  **Filtragem de Integridade:** O dataset foi filtrado para `hospital = True`, alinhando os dados com a defini√ß√£o oficial de "casos hospitalizados".
    8.  **Filtragem de Colunas por Nulidade (com Trade-off):** Como etapa final da transforma√ß√£o, foi implementada uma l√≥gica para remover colunas com mais de 70% de valores nulos. No entanto, foi feito um **trade-off estrat√©gico**: colunas consideradas de alta import√¢ncia anal√≠tica (especificamente `puerpera`, `hematologi`, `sind_down`, `hepatica`, `renal`, `obesidade`), mesmo que acima do threshold de 70%, foram **explicitamente mantidas** no dataset final. Essa decis√£o prioriza a reten√ß√£o de informa√ß√µes potencialmente valiosas para a an√°lise de comorbidades, balanceando a limpeza de dados com as necessidades do neg√≥cio.

* **Carregamento (L):** O carregamento no DuckDB √© feito de forma nativa e otimizada:
    1.  O plano "Lazy" do Polars (`df_final_lazy`) √© "registrado" no DuckDB (`con.register()`).
    2.  Esta opera√ß√£o √© *zero-copy* (via `pyarrow`), "emprestando" os dados na mem√≥ria sem copi√°-los.
    3.  O DuckDB executa o plano e realiza a carga em massa com `CREATE OR REPLACE TABLE srag AS SELECT ...`, o que √© ordens de magnitude mais r√°pido do que m√©todos de inser√ß√£o tradicionais.

### 3. Consultas SQL

As consultas foram projetadas para demonstrar duas abordagens anal√≠ticas diferentes: uma an√°lise "Micro" (focada em dados de Recife) e uma "Macro" (focada em padr√µes epidemiol√≥gicos).

* **Consulta 1: `sql/consulta_1.sql` - An√°lise Micro: Perfil de Gravidade por Faixa Et√°ria em Recife**
    * **Descri√ß√£o:** Esta consulta filtra os casos de SRAG por COVID-19 (2024) para residentes do Recife (c√≥digo `261160`). Ela agrupa os pacientes em faixas et√°rias (`pediatrico`, `adulto`, `idoso`) e calcula o total de casos, a taxa de UTI e a taxa de letalidade para cada grupo.
    * **Justificativa Anal√≠tica:** Sendo a vaga para a Sec. de Sa√∫de do Recife, esta consulta gera um recorte de **alta relev√¢ncia local**. Ela responde: "Qual foi o impacto da COVID-19 em *nossos* mun√≠cipios?". O insight (baseado nos 73 casos) foi claro: o n√∫mero de casos graves foi baixo em 2024, e a letalidade se concentrou de forma extrema no grupo 'idoso' (42.8%), um dado valioso para a vigil√¢ncia epidemiol√≥gica local.

* **Consulta 2: `sql/consulta_2.sql` - An√°lise Macro: Impacto do Ac√∫mulo de Comorbidades na Gravidade (Brasil)**
    * **Descri√ß√£o:** Esta consulta usa uma **Common Table Expression (CTE)** para uma an√°lise de risco avan√ßada. O CTE primeiro calcula o *n√∫mero total de comorbidades* (diabetes, cardiopatia, etc.) para cada paciente de COVID-19 em *todo o dataset*. A consulta principal agrupa os pacientes por essa contagem (0, 1, 2, 3+) e calcula a `taxa_uti` e a `taxa_letalidade` para cada grupo de risco.
    * **Justificativa Anal√≠tica:** Ao contr√°rio da Consulta 1, uma an√°lise de comorbidades no recorte de Recife (N=73) seria estatisticamente fraca. Por isso, esta consulta muda para uma abordagem **"Macro" (n√≠vel nacional)** para identificar um padr√£o epidemiol√≥gico robusto. O insight foi poderoso e claro: **o risco aumenta linearmente com o n√∫mero de comorbidades**. A taxa de letalidade saltou de **10.9%** (para 0 comorbidades) para **27.3%** (para 3+ comorbidades). Esta consulta demonstra o valor do pipeline ETL (que limpou +10 colunas booleanas) e a capacidade de ajustar o escopo da an√°lise (Micro vs. Macro) para gerar insights estatisticamente v√°lidos.